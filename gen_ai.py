# -*- coding: utf-8 -*-
"""GEN AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HGF6HfZ4T7nlfBU3HTIU66fmAGm0S_m4
"""

api = "AIzaSyBooTLulVxwPtZHWrNh-aflm2otoVc_4CA"
model_name = "gemini-2.5-flash"
path = r"/content/education_qa.csv"

import pandas as pd
import google.generativeai as genai
from google.colab import userdata

genai.configure(api_key=api)

model = genai.GenerativeModel(model_name)

df = pd.read_csv(path)

context_text = ""
for _, row in df.iterrows():
    context_text += f"Q: {row['Question']}\nA: {row['Answer']}\n\n"

print(context_text)

def ask_gemini(query):
    if not query.strip():
        return "Please enter a valid question."

    pmt = f"""
    You are a Q&A assistant.

    Answer ONLY using the context below. If you don't know the answer
    say that you don't know and reply refer https://www.metahacks.com/ for
    further clarification, don't try to make up an answer.

    Context:
    {context_text}

    Q: {query}

    Make sure to respond using proper punctuation and simple sentence
    formation such that everyone can understand the response.
    """

    response = model.generate_content(pmt)
    return response.text

print(ask_gemini("What is education"))

print("="*50)
print("Welcome to the GEN AI\nI am AI, Here for general Query answering. You can ask any question related to the education")
print("="*50)

while True:
    user_input = input("You: ")
    if user_input.lower() == "exit":
        break
    try:
        response = ask_gemini(user_input)
        print("GEN AI: ", response)
        print("="*50)
    except Exception as e:
        print("An error occurred: ", e)

import gradio as gr

with gr.Blocks(title="Education Chatbot") as demo:
    gr.Markdown("""
    ## ðŸŽ“ Education Chatbot
    Ask anything about educatoion related and more which you want, i'll give all you need in just a second.
    """)

    with gr.Row():
        question = gr.Textbox(
            label="Your Question",
            placeholder="Ask about education related question which you need to know",
            lines=2
        )

    answer = gr.Textbox(label="Chatbot Answer", lines=6)
    submit = gr.Button("Ask")

    submit.click(
        fn=ask_gemini,
        inputs=question,
        outputs=answer
    )

if __name__ == "__main__":
    demo.launch()

from google.colab import userdata
api = userdata.get('GEMINI')

!pip install langchain_google_genai

import os
from langchain_google_genai import ChatGoogleGenerativeAI

if not os.environ.get("GEMINI_API_KEY"):
    os.environ["GEMINI_API_KEY"] = api

model = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    api_key=os.environ.get("GEMINI_API_KEY")
)

resp = model.invoke("what is cricket")
resp

from IPython.display import Markdown
display(Markdown(resp.content))

print("="*100)
print("Welcome to Gemini 2.5 Flash BOT")
print("Type 'quit' to exit")
print("="*100)

while True:
    user_input = input("You: ")
    if user_input.lower() == "quit":
        print("Goodbye!")
        break
    try:
        response = model.invoke(user_input)
        display(Markdown("Bot: " + response.content))
        print("-"*100)
    except Exception as e:
        print("Error:", str(e))

import pandas as pd
import os
from google.colab import userdata
import google.generativeai as genai
from IPython.display import Markdown

# environmental variables
os.environ["API_KEY"] = userdata.get("GEMINI")
os.environ["DATA_PATH"] = userdata.get("path")
os.environ["MODEL_NAME"] = userdata.get("mid")

genai.configure(api_key=os.environ.get("API_KEY"))

model = genai.GenerativeModel(os.environ.get("MODEL_NAME"))

!pip install faiss-cpu sentence-transformers

import faiss
from sentence_transformers import SentenceTransformer

docx = [f"Q: {q}\nA: {a}" for q, a in zip(df['Question'], df['Answer'])]
docx[:5]

# Embedding
embedder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
emb = embedder.encode(docx)

# FAISS indexing
import numpy as np
faiss.normalize_L2(emb)
index = faiss.IndexFlatL2(emb.shape[1])
index.add(emb)
index

def ask_gemini_rag_eff(query: str) -> str:
    xq = embedder.encode([query])
    faiss.normalize_L2(xq)
    scores, ids = index.search(xq, k=5)

    context_text = "\n\n".join([docx[i] for i in ids[0]])

    prompt = f"""
    You are Natasha, a college Q&A assistant.
    Rules:
    - Answer ONLY using the provided context.
    - If the answer is not present, reply exactly:
      "Please visit https://www.kalke.com for further actions"
    - Do not guess.
    - Respond to greetings.

    Context:
    {context_text}

    Question:
    {query}

    Answer clearly using simple sentence.
    """

    response = model.generate_content(prompt)
    return response.text

embedder.encode(["warm up"])

ask_gemini_rag_eff("what is education")

!pip install google-genai sentence-transformers faiss-cpu

from google.colab import userdata
from google import genai
from sentence_transformers import SentenceTransformer, CrossEncoder
import faiss
import numpy as np
import pandas as pd
from IPython.display import Markdown
import os

from google.colab import userdata
import time

time.sleep(5)   # IMPORTANT: let Colab UI sync
api = userdata.get("GEMINI")
api

from google.colab import userdata
userdata.get("GEMINI")

import os

api = "AIzaSyBooTLulVxwPtZHWrNh-aflm2otoVc_4CA"
os.environ["GEN"] = api

path = "/content/education_qa.csv"
model = "gemini-2.5-flash"

from google.colab import userdata
import os
import time

time.sleep(5)
api = userdata.get("GEMINI")

os.environ["GEN"] = api
path = "/content/education_qa.csv"
model = "gemini-2.5-flash"

api = userdata.get('GEMINI')
os.environ['GEN'] = userdata.get('GEMINI')
path = '/content/education_qa.csv'
model = 'gemini-2.5-flash'

from google import genai
from sentence_transformers import SentenceTransformer, CrossEncoder

client = genai.Client(api_key=api)

embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
rerank_model = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

df = pd.read_csv(path)
display(df.head())

document = [f"Q: {q}\nA: {a}" for q, a in zip(df['Question'], df['Answer'])]

print(document[:5])

embeddings = embedding_model.encode(document)
faiss.normalize_L2(embeddings)

index = faiss.IndexFlatIP(embeddings.shape[1])
index.add(embeddings.astype('float32'))

display(embeddings)
display(index)

chat_history = []

def best_doc_find(query):
    query_vec = embedding_model.encode([query])
    faiss.normalize_L2(query_vec)

    distances, indices = index.search(
        np.array(query_vec).astype('float32'), k=3
    )

    candidates = [document[i] for i in indices[0]]
    pairs = [[query, doc] for doc in candidates]
    scores = rerank_model.predict(pairs)

    best_doc = candidates[np.argmax(scores)]
    return best_doc

def get_rag_response(query):
    global chat_history

    best_doc = best_doc_find(query)
    memory_context = "\n".join(chat_history[-4:])

    prompt = f"""
    History:
    {memory_context}

    Context:
    {best_doc}

    User Question:
    {query}

    Understand the question properly.
    Answer the question briefly using the context
    and history provided.
    Response in simple sentence and formally.
    """

    response = client.models.generate_content(
        model=model,
        contents=prompt
    )

    chat_history.append(f"User: {query}")
    chat_history.append(f"AI: {response.text}")

    return response.text

get_rag_response("is the college located in hyderabad")

while True:
    q = input("You: ")
    if q == "exit":
        break
    display(Markdown(get_rag_response(q)))